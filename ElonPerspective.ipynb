{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-api-python-client\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import pandas as pd \n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyC1WqnJqZ7-qZH-zAwuG24Y1ZpVRXxttLE'\n",
    "\n",
    "client = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    version = \"v1alpha1\",\n",
    "    developerKey=API_KEY,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate the toxicity and profanity for a list of tweets --> returns dataframe of output values\n",
    "def toxicity_score(list_of_tweets):\n",
    "  k = 0 # number of analyzed tweets\n",
    "  minimum_tweet_length = 10  \n",
    "  length = len(list_of_tweets)\n",
    "  # initialise output lists\n",
    "  output = {'Toxicity':[], 'Severe_Toxicity':[], 'Identity_Attack':[], 'Insult':[], 'Profanity':[], 'Threat':[]}\n",
    "\n",
    "  for i in range(length):\n",
    "      # skip any tweets shorter than minimum_tweet_length\n",
    "      if len(list_of_tweets[i]) < minimum_tweet_length:\n",
    "           output['Toxicity'].append(\"-\")\n",
    "           output['Severe_Toxicity'].append(\"-\")\n",
    "           output['Identity_Attack'].append(\"-\")\n",
    "           output['Insult'].append(\"-\")\n",
    "           output['Profanity'].append(\"-\")\n",
    "           output['Threat'].append(\"-\")\n",
    "      else: ## ANALYZE:\n",
    "          analyze_request = {'comment': {'text': list_of_tweets[i]},\n",
    "                             'languages': ['en'],\n",
    "                             'requestedAttributes': {'TOXICITY': {},\n",
    "                                                     'SEVERE_TOXICITY': {},\n",
    "                                                     'IDENTITY_ATTACK': {},\n",
    "                                                     'INSULT': {},\n",
    "                                                     'PROFANITY': {},\n",
    "                                                     'THREAT': {}}}\n",
    "          response = client.comments().analyze(body=analyze_request).execute()\n",
    "          # print(json.dumps(response, indent=4))\n",
    "\n",
    "          output['Toxicity'].append(response['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "          output['Severe_Toxicity'].append(response['attributeScores']['SEVERE_TOXICITY']['summaryScore']['value'])\n",
    "          output['Identity_Attack'].append(response['attributeScores']['IDENTITY_ATTACK']['summaryScore']['value'])\n",
    "          output['Insult'].append(response['attributeScores']['INSULT']['summaryScore']['value'])\n",
    "          output['Profanity'].append(response['attributeScores']['PROFANITY']['summaryScore']['value'])\n",
    "          output['Threat'].append(response['attributeScores']['THREAT']['summaryScore']['value'])\n",
    "\n",
    "          time.sleep(1) # don't exceed 1 request per second \n",
    "          k = k + 1\n",
    "\n",
    "      print(f'1. number of tweets looked at: {i}')\n",
    "      print(f'2. number of tweets analyzed: {k} \\n----------------------------')\n",
    "  return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating the Dataset of Elon Musk Tweets #####\n",
    "\n",
    "##### Combining our two datasets #####\n",
    "\n",
    "# dataframe of all musk tweets till 12.09.2022\n",
    "tweet_df = pd.read_csv('musk_dataset/musk_big_dataset.csv')\n",
    "\n",
    "# newer Musk Tweets from 13.09.2022 - 27.10.2022\n",
    "new_tweet_df = pd.read_csv('musk_dataset/cleandata.csv')\n",
    "\n",
    "# initialise lists\n",
    "output = {'Date Created':[], 'Number of Likes':[], 'Source of Tweet':[], 'Tweets':[]}\n",
    "\n",
    "for i in range(532):\n",
    "    output['Date Created'].append(new_tweet_df['Date'][i])\n",
    "    output['Number of Likes'].append(new_tweet_df['Likes'][i])\n",
    "    output['Source of Tweet'].append(\"-\")\n",
    "    output['Tweets'].append(new_tweet_df['Tweets'][i])\n",
    "\n",
    "new_data = pd.DataFrame(output)\n",
    "\n",
    "data = pd.concat([new_data, tweet_df], \n",
    "                  keys = ['new_data', 'old_data'],\n",
    "                  ignore_index = True)\n",
    "\n",
    "\n",
    "##### Simplify our Dataset #####\n",
    "# simplify the Date Created column in data\n",
    "for i in range(data.shape[0]):\n",
    "    data['Date Created'][i] = re.sub(r\"\\+00:00\", \"\", data['Date Created'][i])\n",
    "\n",
    "# delete the column 'Source of Tweet' \n",
    "del data['Source of Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   thanks\n",
      "1                                               Absolutely\n",
      "2                                 Dear Twitter Advertisers\n",
      "3           Meeting a lot of cool people at Twitter today!\n",
      "4                  Entering Twitter HQ â€“ let that sink in!\n",
      "                               ...                        \n",
      "17964                    That was a total non sequitur btw\n",
      "17965    Great Voltaire quote, arguably better than Twa...\n",
      "17966    I made the volume on the Model S  go to 11.  N...\n",
      "17967    Went to Iceland on Sat to ride bumper cars on ...\n",
      "17968    Please ignore prior tweets, as that was someon...\n",
      "Name: Cleaned Tweets, Length: 17969, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##### Data Cleanup #####\n",
    "def cleantwt (twt):\n",
    "  emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "  twt = re.sub('RT', '', twt)               # remove 'RT' from tweets\n",
    "  twt = re.sub('#[A-Za-z0-9]+', '', twt)    # remove the '#' from the tweets\n",
    "  twt = re.sub('\\\\n', '', twt)              # remove the '\\n' character\n",
    "  twt = re.sub('https?:\\/\\/\\S+', '', twt)   # remove the hyperlinks\n",
    "  twt = re.sub('@[\\S]*', '', twt)           # remove @mentions\n",
    "  twt = re.sub('^[\\s]+|[\\s]+$', '', twt)    # remove leading and trailing whitespaces\n",
    "  twt = re.sub(emoj, '', twt)               # remove emojis\n",
    "  return twt\n",
    "\n",
    "## Cleanup the data\n",
    "Cleaned_Tweets = []\n",
    "for i in range(data.shape[0]):\n",
    "    cleaned_tweet = cleantwt(data['Tweets'][i])\n",
    "    Cleaned_Tweets.append(cleaned_tweet)\n",
    "\n",
    "# -> Dataframe with added column \"Cleaned_Tweets\"\n",
    "data[\"Cleaned Tweets\"] = Cleaned_Tweets\n",
    "\n",
    "print(data['Cleaned Tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. number of tweets looked at: 0\n",
      "2. number of tweets analyzed: 0 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 1\n",
      "2. number of tweets analyzed: 1 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 2\n",
      "2. number of tweets analyzed: 2 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 3\n",
      "2. number of tweets analyzed: 3 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 4\n",
      "2. number of tweets analyzed: 4 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 5\n",
      "2. number of tweets analyzed: 5 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 6\n",
      "2. number of tweets analyzed: 6 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 7\n",
      "2. number of tweets analyzed: 7 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 8\n",
      "2. number of tweets analyzed: 8 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 9\n",
      "2. number of tweets analyzed: 9 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 10\n",
      "2. number of tweets analyzed: 10 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 11\n",
      "2. number of tweets analyzed: 10 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 12\n",
      "2. number of tweets analyzed: 11 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 13\n",
      "2. number of tweets analyzed: 12 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 14\n",
      "2. number of tweets analyzed: 13 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 15\n",
      "2. number of tweets analyzed: 14 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 16\n",
      "2. number of tweets analyzed: 15 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 17\n",
      "2. number of tweets analyzed: 16 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 18\n",
      "2. number of tweets analyzed: 16 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 19\n",
      "2. number of tweets analyzed: 16 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 20\n",
      "2. number of tweets analyzed: 17 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 21\n",
      "2. number of tweets analyzed: 18 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 22\n",
      "2. number of tweets analyzed: 19 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 23\n",
      "2. number of tweets analyzed: 20 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 24\n",
      "2. number of tweets analyzed: 21 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 25\n",
      "2. number of tweets analyzed: 21 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 26\n",
      "2. number of tweets analyzed: 21 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 27\n",
      "2. number of tweets analyzed: 21 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 28\n",
      "2. number of tweets analyzed: 21 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 29\n",
      "2. number of tweets analyzed: 22 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 30\n",
      "2. number of tweets analyzed: 23 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 31\n",
      "2. number of tweets analyzed: 24 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 32\n",
      "2. number of tweets analyzed: 25 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 33\n",
      "2. number of tweets analyzed: 26 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 34\n",
      "2. number of tweets analyzed: 27 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 35\n",
      "2. number of tweets analyzed: 28 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 36\n",
      "2. number of tweets analyzed: 29 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 37\n",
      "2. number of tweets analyzed: 29 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 38\n",
      "2. number of tweets analyzed: 30 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 39\n",
      "2. number of tweets analyzed: 31 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 40\n",
      "2. number of tweets analyzed: 32 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 41\n",
      "2. number of tweets analyzed: 33 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 42\n",
      "2. number of tweets analyzed: 34 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 43\n",
      "2. number of tweets analyzed: 35 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 44\n",
      "2. number of tweets analyzed: 36 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 45\n",
      "2. number of tweets analyzed: 37 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 46\n",
      "2. number of tweets analyzed: 38 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 47\n",
      "2. number of tweets analyzed: 39 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 48\n",
      "2. number of tweets analyzed: 40 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 49\n",
      "2. number of tweets analyzed: 41 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 50\n",
      "2. number of tweets analyzed: 42 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 51\n",
      "2. number of tweets analyzed: 43 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 52\n",
      "2. number of tweets analyzed: 44 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 53\n",
      "2. number of tweets analyzed: 45 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 54\n",
      "2. number of tweets analyzed: 46 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 55\n",
      "2. number of tweets analyzed: 47 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 56\n",
      "2. number of tweets analyzed: 48 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 57\n",
      "2. number of tweets analyzed: 49 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 58\n",
      "2. number of tweets analyzed: 50 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 59\n",
      "2. number of tweets analyzed: 51 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 60\n",
      "2. number of tweets analyzed: 51 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 61\n",
      "2. number of tweets analyzed: 52 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 62\n",
      "2. number of tweets analyzed: 53 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 63\n",
      "2. number of tweets analyzed: 54 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 64\n",
      "2. number of tweets analyzed: 55 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 65\n",
      "2. number of tweets analyzed: 55 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 66\n",
      "2. number of tweets analyzed: 56 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 67\n",
      "2. number of tweets analyzed: 57 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 68\n",
      "2. number of tweets analyzed: 58 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 69\n",
      "2. number of tweets analyzed: 59 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 70\n",
      "2. number of tweets analyzed: 60 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 71\n",
      "2. number of tweets analyzed: 61 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 72\n",
      "2. number of tweets analyzed: 62 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 73\n",
      "2. number of tweets analyzed: 63 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 74\n",
      "2. number of tweets analyzed: 64 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 75\n",
      "2. number of tweets analyzed: 65 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 76\n",
      "2. number of tweets analyzed: 66 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 77\n",
      "2. number of tweets analyzed: 66 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 78\n",
      "2. number of tweets analyzed: 66 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 79\n",
      "2. number of tweets analyzed: 67 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 80\n",
      "2. number of tweets analyzed: 68 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 81\n",
      "2. number of tweets analyzed: 69 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 82\n",
      "2. number of tweets analyzed: 69 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 83\n",
      "2. number of tweets analyzed: 70 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 84\n",
      "2. number of tweets analyzed: 71 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 85\n",
      "2. number of tweets analyzed: 72 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 86\n",
      "2. number of tweets analyzed: 73 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 87\n",
      "2. number of tweets analyzed: 74 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 88\n",
      "2. number of tweets analyzed: 74 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 89\n",
      "2. number of tweets analyzed: 75 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 90\n",
      "2. number of tweets analyzed: 75 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 91\n",
      "2. number of tweets analyzed: 75 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 92\n",
      "2. number of tweets analyzed: 76 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 93\n",
      "2. number of tweets analyzed: 77 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 94\n",
      "2. number of tweets analyzed: 78 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 95\n",
      "2. number of tweets analyzed: 79 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 96\n",
      "2. number of tweets analyzed: 80 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 97\n",
      "2. number of tweets analyzed: 80 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 98\n",
      "2. number of tweets analyzed: 81 \n",
      "----------------------------\n",
      "1. number of tweets looked at: 99\n",
      "2. number of tweets analyzed: 82 \n",
      "----------------------------\n",
      "    Toxicity Severe_Toxicity Identity_Attack    Insult Profanity    Threat\n",
      "0          -               -               -         -         -         -\n",
      "1   0.014577        0.001173        0.003145  0.007893  0.015147  0.007224\n",
      "2   0.030388        0.001373        0.004884  0.015189  0.016958  0.007172\n",
      "3   0.025203        0.001926        0.006956  0.011199  0.021057   0.00936\n",
      "4   0.133882        0.004139        0.004773    0.0258  0.026864  0.073231\n",
      "..       ...             ...             ...       ...       ...       ...\n",
      "95  0.254629        0.016384        0.012441  0.023738  0.096806  0.348043\n",
      "96  0.190293        0.006523        0.006845  0.031367  0.048927  0.113096\n",
      "97         -               -               -         -         -         -\n",
      "98   0.00955        0.000792        0.001952  0.006962  0.011783  0.006874\n",
      "99    0.0232        0.002174        0.006179  0.015037  0.017436  0.007237\n",
      "\n",
      "[100 rows x 6 columns]\n",
      "           Date Created  Number of Likes Source of Tweet  \\\n",
      "0   2022-10-27 16:17:39             7021               -   \n",
      "1   2022-10-27 13:19:25            26737               -   \n",
      "2   2022-10-27 13:08:00           356623               -   \n",
      "3   2022-10-26 21:39:32           195546               -   \n",
      "4   2022-10-26 18:45:58          1043592               -   \n",
      "..                  ...              ...             ...   \n",
      "95  2022-10-19 22:33:20            11060               -   \n",
      "96  2022-10-19 18:46:58           392237               -   \n",
      "97  2022-10-19 18:13:39             2381               -   \n",
      "98  2022-10-19 17:02:48            37029               -   \n",
      "99  2022-10-19 16:59:23            53880               -   \n",
      "\n",
      "                                               Tweets  \\\n",
      "0                               @PeterSchiff ðŸ¤£ thanks   \n",
      "1                               @ZubyMusic Absolutely   \n",
      "2    Dear Twitter Advertisers https://t.co/GMwHmInPAS   \n",
      "3      Meeting a lot of cool people at Twitter today!   \n",
      "4   Entering Twitter HQ â€“ let that sink in! https:...   \n",
      "..                                                ...   \n",
      "95  @westcoastbill Will require truly exceptional ...   \n",
      "96   I will not let you down, no matter what it takes   \n",
      "97                                @DirtyTesLa Awesome   \n",
      "98  We even did a Starlink video call on one airpl...   \n",
      "99                                 Vox Populi Vox Dei   \n",
      "\n",
      "                                       Cleaned Tweets  Toxicity  \\\n",
      "0                                              thanks         -   \n",
      "1                                          Absolutely  0.014577   \n",
      "2                            Dear Twitter Advertisers  0.030388   \n",
      "3      Meeting a lot of cool people at Twitter today!  0.025203   \n",
      "4             Entering Twitter HQ â€“ let that sink in!  0.133882   \n",
      "..                                                ...       ...   \n",
      "95  Will require truly exceptional execution, but ...  0.254629   \n",
      "96   I will not let you down, no matter what it takes  0.190293   \n",
      "97                                            Awesome         -   \n",
      "98  We even did a Starlink video call on one airpl...   0.00955   \n",
      "99                                 Vox Populi Vox Dei    0.0232   \n",
      "\n",
      "   Severe_Toxicity Identity_Attack    Insult Profanity    Threat  \n",
      "0                -               -         -         -         -  \n",
      "1         0.001173        0.003145  0.007893  0.015147  0.007224  \n",
      "2         0.001373        0.004884  0.015189  0.016958  0.007172  \n",
      "3         0.001926        0.006956  0.011199  0.021057   0.00936  \n",
      "4         0.004139        0.004773    0.0258  0.026864  0.073231  \n",
      "..             ...             ...       ...       ...       ...  \n",
      "95        0.016384        0.012441  0.023738  0.096806  0.348043  \n",
      "96        0.006523        0.006845  0.031367  0.048927  0.113096  \n",
      "97               -               -         -         -         -  \n",
      "98        0.000792        0.001952  0.006962  0.011783  0.006874  \n",
      "99        0.002174        0.006179  0.015037  0.017436  0.007237  \n",
      "\n",
      "[100 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "##### Rating the Toxicity of Elon Musk Tweets #####  \n",
    "tweet_list = list(data.head(100)['Cleaned Tweets'])\n",
    "\n",
    "### rate the toxicity ### \n",
    "scores = toxicity_score(tweet_list)\n",
    "print(scores)\n",
    "\n",
    "# write the scores into the dataframe\n",
    "data = pd.concat([data.head(100), scores], axis=1, join='inner')\n",
    "print(data)\n",
    "data.to_csv('Elon_Musk_Twitter_Toxicity.csv', float_format=\"%.10f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                thanks\n",
      "1                                            Absolutely\n",
      "2                              Dear Twitter Advertisers\n",
      "3        Meeting a lot of cool people at Twitter today!\n",
      "4               Entering Twitter HQ â€“ let that sink in!\n",
      "                            ...                        \n",
      "95    Will require truly exceptional execution, but ...\n",
      "96     I will not let you down, no matter what it takes\n",
      "97                                              Awesome\n",
      "98    We even did a Starlink video call on one airpl...\n",
      "99                                   Vox Populi Vox Dei\n",
      "Name: Cleaned Tweets, Length: 100, dtype: object\n",
      "Maximum Toxicity had Tweet Nr.  36: While itâ€™s true that Kasparov is almost as good at playing chess as my iPhone, he is otherwise an idiot \t score: 0.8115627\n",
      "Maximum Profanity had Tweet Nr. 64: un ass ailable logic! \t score: 0.5150164\n"
     ]
    }
   ],
   "source": [
    "### Visualization of the most Toxic / Profane Tweets ###\n",
    "data2 = pd.read_csv('Elon_Musk_Twitter_Toxicity.csv')\n",
    "print(data2['Cleaned Tweets'])\n",
    "\n",
    "\n",
    "max_tox = 1\n",
    "max_prof = 1\n",
    "for i in range(100):\n",
    "    if (data2['Toxicity'][i] == \"-\"):\n",
    "        i = i + 1\n",
    "    else: \n",
    "        if (data2['Toxicity'][i] > data2['Toxicity'][max_tox]):\n",
    "              max_tox = i\n",
    "        if (data2['Profanity'][i] > data2['Profanity'][max_prof]):\n",
    "              max_prof = i\n",
    "\n",
    "print(f'Maximum Toxicity had Tweet Nr.  {max_tox}: {data2[\"Cleaned Tweets\"][max_tox]} \\t score: {data2[\"Toxicity\"][max_tox]}')\n",
    "print(f'Maximum Profanity had Tweet Nr. {max_prof}: {data2[\"Cleaned Tweets\"][max_prof]} \\t score: {data2[\"Profanity\"][max_prof]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
